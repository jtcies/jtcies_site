## part 1: how to do this? 

- Deloitte article
  - prioritizing on the ground decision-making
  - data for advancing equity
- [michael quinn patton][1]
  - The evaluatorâ€™s primary function in the team is to elucidate team discussions with evaluative questions, data and logic, and to facilitate data-based assessments and decision-making in the unfolding and developmental processes of innovation.
- where are placed that this has worked?
  - examples:
    - Atlanta housing article?
    - Pittsburgh abuse warning detector
      - researchers created a model to estimate the risk of abuse for a child; 'offers up a second opinion'
      - based on prior calls and other administrative data
      - gives results as probability
      - presents information to decision-makers rather making decisions for practitioners
      - elements
        - innovation is the algorithm; however, there must have been a process to determine that just letting humans do the assessment alone waasn't working for some reason
        - clear goals: improve the rate at which department accurately assesses the risk of abuse
        - intersects directly in decision-making process; very tough decisions made many times a day; goal is to enchance decision-making
        - rather just create a report that says, 'here are the risk factors associated with abuse', they went further to interesect directly in the decision-making process of practitioners
    - Philly pre-k
  - common elements
    - focus on parternship
    - prioritizing decision-making, implementation
    - shared understanding of goals, activities, outcomes

## part 2: go beyond metrics

- data science has power to automate, deprioritize performance measures, tracking
  - shift focus to implementation
- metrics are important, have their place
  - we can set up systems and processes to track these things in real time
  - data science makes this (somewhat) trivial
  - can go beyond performance metrics
- the hard part is getting clear onw hat it is we should be tracking
  - in doing this, we have the opportunity to go deeper
  - need to get explicit about what we're doing, what we're trying to achieve
- dig deeper into implementation
  - real-time feedback, rather than relying on academics/contracted reseachers
  - requires shared understanding of goals, activities, outcomes

## part 3: developing shared understandings

- how to develop shared understandings
  - start with extracting the ideal
  - use data to examine what is actually happening
- provide opportunities for parternship, analyzing together
- start with simple visualizations and buidl up to more complex models
- this is an idealzied version; still working on implementing this cycle


[1]: https://nonprofitquarterly.org/2006/03/21/evaluation-for-the-way-we-work/
[2]: https://www.nytimes.com/2018/01/02/magazine/can-an-algorithm-tell-when-kids-are-in-danger.html