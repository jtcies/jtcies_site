---
title: Does it rain more on Tuesdays (with informative priors)?
date: 2020-04-30
---



<p>A couple of years ago, I wrote about rain on different days of the week in Philadelphia. I was annoyed because it felt like every time I took out the trash, it was raining. Our trash day is Wednesdays, so I take it out on Tuesday nights. When I did that analysis, I found that, sure enough, it did rain more on Tuesdays! Maybe not that interesting - it’s gotta rain more on some day, right? The surprising thing though was that the confidence interval for Tuesday did not include 0. So I could reject my null hypothesis that it rained the same amount on each day and say my results were statistically significant.</p>
<p><a href="https://www.cambridge.org/core/services/aop-cambridge-core/content/view/FFAB96BDC5EE3C64B144ECF8F90F31E9/S1138741619000416a.pdf/bayesian_estimation_with_informative_priors_is_indistinguishable_from_data_falsification.pdf">This</a> article exmemplifies a typical argument in the frequent vs Bayesian, even if the title - ‘Bayesian Estimation with Informative Priors is Indistinguishable from Data Falsification’ - is somewhat provactive. The argument goes that using priors is basically just maniuplating the data to get the results you want. This puts Bayseians in something of a catch-22: if you use informative priors, you could be seen as maniuplating your data to find the outcomes you want; if you use uninformative priors, then you’re just doing frequentist statistics!</p>
<p>Reading this recently made me think about the rain on Tuesdays article. At the time, my view was that this was a fluke of randomness; occassionally you will find a result that looks like there is something going on when there isn’t. Obviously the day of the week doesn’t impact the weather. I was really just joking when I did it the first time and didn’t expect to find anything. However, maybe a Bayesian approach will give us an answer that we know is more in line with reality.</p>
<pre class="r"><code>library(tidyverse)
library(lubridate)
library(here)
library(rstanarm)
library(jtcr)
library(broom)
library(pROC)

theme_set(theme_jtc())


# use same data that goes up to September 2018
weather &lt;- &quot;https://jtcies.com/data/phl_weather_20090925-20180924.csv&quot; %&gt;% 
  read_csv(guess_max = 2000) %&gt;%
  janitor::clean_names()

weather_processed &lt;- weather %&gt;%
    mutate(
        dow = as.factor(wday(date)),
        rain = as.integer(prcp &gt; 0)
    )</code></pre>
<p>Below is the percentage of days that it rained in Philadelphia by day of the week:</p>
<pre class="r"><code>weather_processed %&gt;%
    group_by(dow) %&gt;%
    summarise(
        n = n(),
        `percent rainy days` = mean(rain)
    ) %&gt;%
    knitr::kable()</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">dow</th>
<th align="right">n</th>
<th align="right">percent rainy days</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">1</td>
<td align="right">469</td>
<td align="right">0.2857143</td>
</tr>
<tr class="even">
<td align="left">2</td>
<td align="right">469</td>
<td align="right">0.3198294</td>
</tr>
<tr class="odd">
<td align="left">3</td>
<td align="right">469</td>
<td align="right">0.3859275</td>
</tr>
<tr class="even">
<td align="left">4</td>
<td align="right">469</td>
<td align="right">0.3262260</td>
</tr>
<tr class="odd">
<td align="left">5</td>
<td align="right">469</td>
<td align="right">0.3411514</td>
</tr>
<tr class="even">
<td align="left">6</td>
<td align="right">470</td>
<td align="right">0.3063830</td>
</tr>
<tr class="odd">
<td align="left">7</td>
<td align="right">469</td>
<td align="right">0.3176972</td>
</tr>
</tbody>
</table>
<div id="comparing-models" class="section level2">
<h2>Comparing Models</h2>
<p>We’ll fit four models to see how the priors impact the coefficients. My assumption is that the day of the week has no impact on whether it rains so models that produce coefficients closer to zero should be closer to what we’d expect. The first model is a likelihood model and the next three are Bayesian models with increasingly informative priors.</p>
<pre class="r"><code>mod1 &lt;- glm(
    rain ~ dow,
    family = binomial(link = &quot;logit&quot;),
    data = weather_processed
)

mod2 &lt;- stan_glm(
    rain ~ 1 + dow,
    family = binomial(link = &quot;logit&quot;),
    data = weather_processed,
    prior = normal(0, 0.5),
    prior_intercept = normal(-0.8, 1)
)

mod3 &lt;- stan_glm(
    rain ~ 1 + dow,
    family = binomial(link = &quot;logit&quot;),
    data = weather_processed,
    prior = normal(0, 0.25),
    prior_intercept = normal(-0.8, 1)
)

mod4 &lt;- stan_glm(
    rain ~ 1 + dow,
    family = binomial(link = &quot;logit&quot;),
    data = weather_processed,
    prior = normal(0, 0.1),
    prior_intercept = normal(-0.8, 1)
)</code></pre>
<pre class="r"><code>extract_coef &lt;- function(x) {

    labels &lt;-  c(&quot;Intercept&quot;, as.character(wday(2:7, label = TRUE)))
    priors &lt;- prior_summary(x)$prior
    dist &lt;- priors$dist
    mu &lt;- unique(priors$location)
    sigma &lt;- unique(priors$scale)

    if (is.null(priors)) {
            prior_label &lt;- &quot;no prior&quot;
    } else {
        prior_label &lt;- paste0(dist, &quot;(&quot;, mu, &quot;, &quot;, sigma, &quot;)&quot;)
    }

    tidy(x) %&gt;%
        mutate(param = labels,
               prior = prior_label)
}

results &lt;- list(mod1, mod2, mod3, mod4) %&gt;%
    map_dfr(extract_coef, .id = &quot;model&quot;)

results %&gt;%
    mutate(param = fct_rev(fct_inorder(param)),
           prior = fct_rev(fct_inorder(prior))) %&gt;%
    ggplot(aes(param, estimate, color = prior)) +
        geom_point(
            size = 2,
            position = position_dodge(width = 0.5)
        ) +
        geom_linerange(
            aes(ymin = estimate - 1.96 * std.error,
                ymax = estimate + 1.96 * std.error),
            position = position_dodge(width = 0.5)
        ) +
        coord_flip() +
        color_jtc(&quot;color&quot;) +
        guides(color = guide_legend(reverse = TRUE)) + 
        labs(
            title = &quot;Informative priors get us closer to what we expect&quot;,
            subtitle = &quot;bars represent 95% interval around estimates&quot;,
            y = &quot;coefficient&quot;,
            x = &quot;parameter&quot;
        )</code></pre>
<p><img src="/post/revisiting-rain-on-tuesdays_files/figure-html/unnamed-chunk-4-1.png" width="960" style="display: block; margin: auto;" /></p>
<p>The informative priors don’t substantially change the predictions in this example. There are very slight differences with more informative priors having a negative impact on in-sample fit and a better impact on out-of-sample fit.</p>
<pre class="r"><code>calc_roc &lt;- function(x) {
    preds &lt;- augment(x, type.predict = &quot;response&quot;)

    area_under &lt;- auc(roc(preds, rain, .fitted,
                          direction = &quot;&lt;&quot;,
                          levels = c(0, 1)))

    threshold &lt;- seq(0, 1, by = 0.01)

    pred_levels &lt;- preds %&gt;%
        nest(data = everything()) %&gt;%
        crossing(threshold) %&gt;%
        unnest(data) %&gt;%
        mutate(
            tp = .fitted &gt;= threshold &amp; rain == 1,
            fp = .fitted &gt; threshold &amp; rain == 0
        )

    pred_levels %&gt;%
        group_by(threshold) %&gt;%
        summarise(
            tpr = sum(tp) / sum(rain == 1),
            fpr = sum(fp) / sum(rain == 0)
        ) %&gt;%
        mutate(area_under = as.numeric(area_under))
}

list(mod1, mod2, mod3, mod4) %&gt;%
    map_dfr(calc_roc, .id = &quot;model&quot;) %&gt;%
    left_join(
        results %&gt;%
            distinct(model, prior),
        by = &quot;model&quot;
    ) %&gt;%
    mutate(
        prior = paste0(prior, &quot; | AUC: &quot;, round(area_under, 3)),
        prior = fct_rev(fct_inorder(prior))
    ) %&gt;%
    ggplot(aes(fpr, tpr, color = prior)) +
        geom_line(size = 2, alpha = 0.5) +
        expand_limits(y = c(0, 1), x = c(0, 1)) +
        color_jtc(&quot;color&quot;) +
        guides(color = guide_legend(reverse = TRUE)) +
        labs(title = &quot;Most informative priors perform worst on in-sample predictions&quot;,
             x = &quot;False positive rate&quot;,
             y = &quot;True positive rate&quot;)</code></pre>
<p><img src="/post/revisiting-rain-on-tuesdays_files/figure-html/unnamed-chunk-5-1.png" width="960" style="display: block; margin: auto;" /></p>
<div id="informative-priors-perform-best-on-out-of-sample-predictions." class="section level4">
<h4>Informative priors perform best on out of sample predictions.</h4>
<pre class="r"><code>loo_compare(loo(mod2), loo(mod3), loo(mod4)) %&gt;%
    knitr::kable()</code></pre>
<table>
<thead>
<tr class="header">
<th></th>
<th align="right">elpd_diff</th>
<th align="right">se_diff</th>
<th align="right">elpd_loo</th>
<th align="right">se_elpd_loo</th>
<th align="right">p_loo</th>
<th align="right">se_p_loo</th>
<th align="right">looic</th>
<th align="right">se_looic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>mod4</td>
<td align="right">0.0000000</td>
<td align="right">0.000000</td>
<td align="right">-2073.552</td>
<td align="right">19.57960</td>
<td align="right">3.726136</td>
<td align="right">0.0514615</td>
<td align="right">4147.103</td>
<td align="right">39.15921</td>
</tr>
<tr class="even">
<td>mod3</td>
<td align="right">-0.2490228</td>
<td align="right">1.246672</td>
<td align="right">-2073.801</td>
<td align="right">19.69632</td>
<td align="right">5.890217</td>
<td align="right">0.0784900</td>
<td align="right">4147.601</td>
<td align="right">39.39265</td>
</tr>
<tr class="odd">
<td>mod2</td>
<td align="right">-0.3743849</td>
<td align="right">1.864321</td>
<td align="right">-2073.926</td>
<td align="right">19.79567</td>
<td align="right">6.564268</td>
<td align="right">0.0862961</td>
<td align="right">4147.852</td>
<td align="right">39.59135</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<p>The point of looking at this was to see if informative priors helped us in this particular case. They did, but where does that end? How do you know in more ambiguous cases? I don’t know.</p>
<p>It seems to me that informative priors are really only data falsification if you don’t have good reasons to use them. I get the sense that those who advocate against informative priors are concerned that researchers are fishing for a particular result, either intentionally or unintentionally. We had a good reason to use informative priors here - the day of the week doesn’t do shit to the weather! It’s ridiculous to think that it does.</p>
<p>This also shows how throwing a bunch of stuff at your model without some form of regularization or priors will occassionally find patterns that don’t exist in reality.</p>
<p>I also wonder if there is a distinction between priors which push the paramers toward or away form a null result.</p>
<p>Mostly though, the techniques one uses should be based on the type of question you’re trying to answer, the data you have, and what an answer to that quesiton means for decision-making.</p>
</div>
